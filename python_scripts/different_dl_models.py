# -*- coding: utf-8 -*-
"""different DL models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jw7M7UqM8q0CInZndjWS-Q1ihfq_pW6M
"""
set_seed(0)

class NeuralNetwork1(nn.Module):
    def __init__(self):
        super(NeuralNetwork1, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)


class NeuralNetwork2(nn.Module):
    def __init__(self):
        super(NeuralNetwork2, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 150),
            nn.ReLU(),
            nn.Linear(150, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)


class NeuralNetwork3(nn.Module):
    def __init__(self):
        super(NeuralNetwork3, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 100),
            nn.LeakyReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)


class NeuralNetwork4(nn.Module):
    def __init__(self):
        super(NeuralNetwork4, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 200),
            nn.ReLU(),
            nn.Linear(200, 150),
            nn.ReLU(),
            nn.Linear(150, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)


class NeuralNetwork5(nn.Module):
    def __init__(self):
        super(NeuralNetwork5, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 100),
            nn.Tanh(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)


class NeuralNetwork6(nn.Module):
    def __init__(self):
        super(NeuralNetwork6, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 200),
            nn.Tanh(),
            nn.Linear(200, 100),
            nn.ReLU(),
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

                
# one hidden layer and relu
class Combined1(nn.Module):
    def __init__(self):
        super(Combined1, self).__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 100),
            nn.ReLU(),
            nn.Linear(100, 5),
            nn.Sigmoid()
        )

        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

# Two hidden layers and relu
class Combined2(nn.Module):
    def __init__(self):
        super(Combined2, self).__init__()

        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 150),
            nn.ReLU(),
            nn.Linear(150, 75),
            nn.ReLU(),
            nn.Linear(75, 5),
            nn.Sigmoid()
        )

        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)

# Three hidden layers and leaky relu
class Combined3(nn.Module):
    def __init__(self):
        super(Combined3, self).__init__()

        self.layers = nn.Sequential(
            nn.Linear(num_input_units, 200),
            nn.LeakyReLU(),
            nn.Linear(200, 100),
            nn.LeakyReLU(),
            nn.Linear(100, 50),
            nn.LeakyReLU(),
            nn.Linear(50, 5),
            nn.Sigmoid()
        )

        self.initialize_weights()

    def forward(self, x):
        return self.layers(x)

    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
