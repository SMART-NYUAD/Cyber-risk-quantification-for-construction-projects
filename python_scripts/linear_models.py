# -*- coding: utf-8 -*-
"""try.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tnjmody0lXOx7MvXkkxW9Ez6-r9EzChd
"""


set_seed(0)

# Define the model
linear = LinearRegression()

# Train the model
linear.fit(train_val_x_np, train_val_y_np)

# Predict on test set
linear_pred = linear.predict(test_x_np)

# Compute metrics
mse_linear = mean_squared_error(test_y_np, linear_pred)
rmse_linear = np.sqrt(mse_linear)
mae_linear = mean_absolute_error(test_y_np, linear_pred)
r2_linear = r2_score(test_y_np, linear_pred)

# Add Linear Regression metrics to the dictionary
metrics_dict['Linear Regression'] = {
    'MSE': mse_linear,
    'RMSE': rmse_linear,
    'MAE': mae_linear,
    'R²': r2_linear
}

# Print metrics
print(f"Metrics for Linear Regression:")
print(f"Mean Squared Error (MSE): {mse_linear:.3e}")
print(f"Root Mean Squared Error (RMSE): {rmse_linear:.3e}")
print(f"Mean Absolute Error (MAE): {mae_linear:.3e}")
print(f"R-squared (R²): {round(r2_linear,3)}\n")


# Define the model
ridge = Ridge()

# Define the grid of hyperparameters 'alpha'
param_grid = {'alpha': np.logspace(-4, 4, 100)}

# Perform grid search with cross-validation
ridge_cv = GridSearchCV(ridge, param_grid, cv=5)
ridge_cv.fit(train_val_x_np, train_val_y_np)

# Print best alpha
print("Best alpha: ", ridge_cv.best_params_)

# Predict on test set
ridge_pred = ridge_cv.predict(test_x_np)

# Compute metrics
mse_ridge = mean_squared_error(test_y_np, ridge_pred)
rmse_ridge = np.sqrt(mse_ridge)
mae_ridge = mean_absolute_error(test_y_np, ridge_pred)
r2_ridge = r2_score(test_y_np, ridge_pred)

metrics_dict['Ridge Regression'] = {
    'MSE': mse_ridge,
    'RMSE': rmse_ridge,
    'MAE': mae_ridge,
    'R²': r2_ridge
}

# Print metrics
print(f"Metrics for Ridge Regression:")
print(f"Mean Squared Error (MSE): {mse_ridge:.3e}")
print(f"Root Mean Squared Error (RMSE): {rmse_ridge:.3e}")
print(f"Mean Absolute Error (MAE): {mae_ridge:.3e}")
print(f"R-squared (R²): {round(r2_ridge,3)}\n")

from sklearn.linear_model import Lasso

# Define the model
lasso = Lasso()

# Define the grid of hyperparameters 'alpha'
param_grid = {'alpha': np.logspace(-4, 4, 100)}

# Perform grid search with cross-validation
lasso_cv = GridSearchCV(lasso, param_grid, cv=5)
lasso_cv.fit(train_val_x_np, train_val_y_np)

# Print best alpha
print("Best alpha: ", lasso_cv.best_params_)

# Predict on test set
lasso_pred = lasso_cv.predict(test_x_np)

# Compute metrics
mse_lasso = mean_squared_error(test_y_np, lasso_pred)
rmse_lasso = np.sqrt(mse_lasso)
mae_lasso = mean_absolute_error(test_y_np, lasso_pred)
r2_lasso = r2_score(test_y_np, lasso_pred)

# Add Lasso Regression metrics to the dictionary
metrics_dict['Lasso Regression'] = {
    'MSE': mse_lasso,
    'RMSE': rmse_lasso,
    'MAE': mae_lasso,
    'R²': r2_lasso
}

# Print metrics
print(f"Metrics for Lasso Regression:")
print(f"Mean Squared Error (MSE): {mse_lasso:.3e}")
print(f"Root Mean Squared Error (RMSE): {rmse_lasso:.3e}")
print(f"Mean Absolute Error (MAE): {mae_lasso:.3e}")
print(f"R-squared (R²): {round(r2_lasso,3)}\n")

# Commented out IPython magic to ensure Python compatibility.
# %%sql
# select * from information_schema.columns;

